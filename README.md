구현 리스트  
RNN   
LSTM   
Transformer   
T5   
BERT   
GPT   
Vision Transformer   
MSTC   
Alphago zero   
Distillation   
Quantization   
Tiemerging   
llm in flash   
Mamba   
CLIP   


https://huggingface.co/collections/fffiloni/sora-reference-papers-65d0c8d4891646a27b84c4a8

[1706.03762] Attention Is All You Need  
NeroIPS2022  
NeroIPS2023  
NeroIPS2024  



모든걸 삼켜버릴 BlackMamba! SSM과 MoE의 결합으로 언어 모델링의 새로운 지평을 여는 모델
BlackMamba는 상태 공간 모델(SSM)과 전문가 혼합(MoE) 모델의 장점을 결합한 새로운 언어 모델로, 언어 모델링과 긴 시퀀스 처리에서 뛰어난 성능을 보여줍니다. 이 모델은 시퀀스 길이에 따른 선형 시간 및 메모리 복잡성을 달성하고, 추론 및 훈련의 효율성을 개선합니다. 300억 개의 토큰으로 구성된 데이터 세트에서 훈련된 BlackMamba는 모든 가중치와 코드를 오픈소스로 제공하며, SSM과 MoE의 결합된 장점을 통해 언어 모델링 분야에 새로운 기준을 제시합니다.
https://huggingface.co/papers/2402.01771


ASPIRE: 대규모 언어 모델의 신뢰성과 정확성 향상을 위한 새로운 프레임워크
대규모 언어 모델의 신뢰성 문제를 해결하기 위해 ASPIRE라는 새로운 프레임워크가 개발되었습니다. ASPIRE는 LLM을 질문 답변(QA) 작업에 특화하여 미세 조정하고, 답변에 대한 신뢰도 점수를 제공함으로써 선택적 예측 기능을 개선합니다. 실험 결과, ASPIRE는 여러 QA 데이터 세트에서 기존의 선택적 예측 방법보다 우수한 성능을 나타냈으며, LLM의 응용 가능성을 넓히는 중요한 발전을 보여줍니다.
https://blog.research.google/2024/01/introducing-aspire-for-selective.html


[PaperList for implemetation]  
aaa